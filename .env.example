# LogAI Configuration
# Copy this file to .env and fill in your values

# === LLM Provider Configuration ===
# Choose your LLM provider: anthropic (default), openai, ollama, or github-copilot
LOGAI_LLM_PROVIDER=anthropic

# Anthropic API Key (required if using Anthropic)
# Get your key from: https://console.anthropic.com/
LOGAI_ANTHROPIC_API_KEY=your-anthropic-api-key-here

# OpenAI API Key (required if using OpenAI)
# Get your key from: https://platform.openai.com/api-keys
LOGAI_OPENAI_API_KEY=your-openai-api-key-here

# LLM Model Selection (optional, defaults to provider's recommended model)
LOGAI_ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
LOGAI_OPENAI_MODEL=gpt-4-turbo-preview

# === GitHub Copilot Configuration ===
# Use your existing GitHub Copilot subscription for access to 25+ models
# Requires authentication: logai auth login
# No API key needed - uses OAuth token stored in ~/.local/share/logai/auth.json

# GitHub Copilot model selection (required if using github-copilot provider)
# Popular options:
#   - claude-opus-4.5, claude-sonnet-4.5, claude-haiku-4.5
#   - gpt-4o, gpt-4o-mini, gpt-5.2
#   - gemini-2.5-pro, gemini-2.5-flash
LOGAI_GITHUB_COPILOT_MODEL=gpt-4o-mini

# GitHub Copilot API base URL (default: https://api.githubcopilot.com)
LOGAI_GITHUB_COPILOT_API_BASE=https://api.githubcopilot.com

# === Ollama Configuration (Local Models) ===
# Base URL for Ollama API (default: http://localhost:11434)
LOGAI_OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use (must support function calling)
# Recommended: llama3.1:8b, llama3.1:70b, mistral
LOGAI_OLLAMA_MODEL=llama3.1:8b

# === AWS Configuration ===
# AWS Region for CloudWatch Logs (can be overridden with --aws-region)
AWS_DEFAULT_REGION=us-east-1

# AWS Credentials (use one of the following methods)
# Method 1: Direct credentials
AWS_ACCESS_KEY_ID=your-aws-access-key-id
AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key

# Method 2: Use AWS profile (can be overridden with --aws-profile)
# AWS_PROFILE=your-aws-profile-name

# Note: Command-line arguments take precedence over these environment variables
# Example: logai --aws-profile prod --aws-region us-west-2

# === LogAI Application Settings ===
# Enable PII sanitization (default: true)
# When enabled, sensitive data like emails, IPs, API keys are redacted before sending to LLM
LOGAI_PII_SANITIZATION_ENABLED=true

# === UI Settings ===
# Show log groups sidebar by default (true/false, default: true)
# The sidebar can always be toggled with /logs command
LOGAI_LOG_GROUPS_SIDEBAR_VISIBLE=true

# Cache directory for storing fetched logs (default: ~/.logai/cache)
LOGAI_CACHE_DIR=~/.logai/cache

# Maximum cache size in MB (default: 500)
LOGAI_CACHE_MAX_SIZE_MB=500

# Cache TTL in seconds for historical logs (default: 86400 = 24 hours)
LOGAI_CACHE_TTL_SECONDS=86400

# === Agent Behavior Settings ===
# Maximum number of tool calls allowed in a single conversation turn
# Prevents infinite loops while allowing complex multi-step queries
# Range: 1-100, Default: 10
# Increase for complex analysis (15-25), decrease for faster failures (5)
LOGAI_MAX_TOOL_ITERATIONS=10

# Enable automatic retry behavior when queries return empty results
# When enabled, agent will try 2-3 alternative approaches before giving up
# Default: true
LOGAI_AUTO_RETRY_ENABLED=true

# Enable intent detection to prevent "I'll try X" without actually doing X
# When enabled, system detects when agent states intent but doesn't execute
# Default: true
LOGAI_INTENT_DETECTION_ENABLED=true

# === Logging Configuration ===
# Log level: DEBUG, INFO, WARNING, ERROR (default: INFO)
LOGAI_LOG_LEVEL=INFO

# Log file location (optional, defaults to stderr only)
# LOGAI_LOG_FILE=~/.logai/logai.log

# === Context Window Management ===
# Model-specific context window size (auto-detected if not set)
# Claude 3.5 Sonnet: 200000, GPT-4 Turbo: 128000, Ollama varies by model
# LOGAI_CONTEXT_WINDOW_SIZE=200000

# Safety buffer to prevent context overflow (default: 5000 tokens)
LOGAI_CONTEXT_WINDOW_BUFFER=5000

# Maximum tokens for a single tool result before caching (default: 50000)
LOGAI_MAX_RESULT_TOKENS=50000

# Maximum tokens for conversation history (default: 80000)
LOGAI_MAX_HISTORY_TOKENS=80000

# Maximum tokens for system prompt (default: 10000)
LOGAI_MAX_SYSTEM_PROMPT_TOKENS=10000

# Tokens reserved for LLM response (default: 8000)
LOGAI_RESERVE_RESPONSE_TOKENS=8000

# === Result Caching ===
# Enable caching of large tool results (default: true)
LOGAI_ENABLE_RESULT_CACHING=true

# Enable incremental fetching of cached results (default: true)
LOGAI_ENABLE_INCREMENTAL_FETCH=true

# Token threshold for caching results (default: 10000)
# Results larger than this will be cached instead of added to context
LOGAI_CACHE_LARGE_RESULTS_THRESHOLD=10000

# Maximum events per cached result chunk (default: 100)
LOGAI_MAX_EVENTS_PER_CHUNK=100

# === History Management ===
# Enable automatic pruning of old messages (default: true)
LOGAI_ENABLE_HISTORY_PRUNING=true

# Number of recent messages to preserve when pruning (default: 20)
LOGAI_HISTORY_SLIDING_WINDOW_MESSAGES=20

# Enable summarization of pruned history (future feature, default: false)
LOGAI_ENABLE_HISTORY_SUMMARIZATION=false

# === Cached Result Agent Guidance ===
# Enable automatic guidance for fetching cached chunks (default: true)
# When enabled, agent automatically fetches chunks after a result is cached
LOGAI_ENABLE_AUTO_FETCH_GUIDANCE=true

# Initial chunk size when fetching cached results (default: 100)
# Range: 50-200 events
LOGAI_INITIAL_CHUNK_SIZE=100

# Maximum automatic chunk fetches per conversation turn (default: 3)
# Prevents runaway fetching behavior
LOGAI_MAX_AUTO_CHUNK_FETCHES=3

# === Context Allocation Strategy ===
# How to allocate context budget between history and results
# Options: adaptive (default), history-focused, result-focused
# - adaptive: Balanced allocation, adjusts based on conversation
# - history-focused: Prioritize preserving full conversation history
# - result-focused: Prioritize complete tool results
LOGAI_CONTEXT_ALLOCATION_STRATEGY=adaptive
