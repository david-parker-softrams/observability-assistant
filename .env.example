# LogAI Configuration
# Copy this file to .env and fill in your values

# === LLM Provider Configuration ===
# Choose your LLM provider: anthropic (default), openai, ollama, or github-copilot
LOGAI_LLM_PROVIDER=anthropic

# Anthropic API Key (required if using Anthropic)
# Get your key from: https://console.anthropic.com/
LOGAI_ANTHROPIC_API_KEY=your-anthropic-api-key-here

# OpenAI API Key (required if using OpenAI)
# Get your key from: https://platform.openai.com/api-keys
LOGAI_OPENAI_API_KEY=your-openai-api-key-here

# LLM Model Selection (optional, defaults to provider's recommended model)
LOGAI_ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
LOGAI_OPENAI_MODEL=gpt-4-turbo-preview

# === GitHub Copilot Configuration ===
# Use your existing GitHub Copilot subscription for access to 25+ models
# Requires authentication: logai auth login
# No API key needed - uses OAuth token stored in ~/.local/share/logai/auth.json

# GitHub Copilot model selection (required if using github-copilot provider)
# Popular options:
#   - claude-opus-4.5, claude-sonnet-4.5, claude-haiku-4.5
#   - gpt-4o, gpt-4o-mini, gpt-5.2
#   - gemini-2.5-pro, gemini-2.5-flash
LOGAI_GITHUB_COPILOT_MODEL=gpt-4o-mini

# GitHub Copilot API base URL (default: https://api.githubcopilot.com)
LOGAI_GITHUB_COPILOT_API_BASE=https://api.githubcopilot.com

# === Ollama Configuration (Local Models) ===
# Base URL for Ollama API (default: http://localhost:11434)
LOGAI_OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use (must support function calling)
# Recommended: llama3.1:8b, llama3.1:70b, mistral
LOGAI_OLLAMA_MODEL=llama3.1:8b

# === AWS Configuration ===
# AWS Region for CloudWatch Logs (can be overridden with --aws-region)
AWS_DEFAULT_REGION=us-east-1

# AWS Credentials (use one of the following methods)
# Method 1: Direct credentials
AWS_ACCESS_KEY_ID=your-aws-access-key-id
AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key

# Method 2: Use AWS profile (can be overridden with --aws-profile)
# AWS_PROFILE=your-aws-profile-name

# Note: Command-line arguments take precedence over these environment variables
# Example: logai --aws-profile prod --aws-region us-west-2

# === LogAI Application Settings ===
# Enable PII sanitization (default: true)
# When enabled, sensitive data like emails, IPs, API keys are redacted before sending to LLM
LOGAI_PII_SANITIZATION_ENABLED=true

# Cache directory for storing fetched logs (default: ~/.logai/cache)
LOGAI_CACHE_DIR=~/.logai/cache

# Maximum cache size in MB (default: 500)
LOGAI_CACHE_MAX_SIZE_MB=500

# Cache TTL in seconds for historical logs (default: 86400 = 24 hours)
LOGAI_CACHE_TTL_SECONDS=86400

# === Agent Behavior Settings ===
# Maximum number of tool calls allowed in a single conversation turn
# Prevents infinite loops while allowing complex multi-step queries
# Range: 1-100, Default: 10
# Increase for complex analysis (15-25), decrease for faster failures (5)
LOGAI_MAX_TOOL_ITERATIONS=10

# Enable automatic retry behavior when queries return empty results
# When enabled, agent will try 2-3 alternative approaches before giving up
# Default: true
LOGAI_AUTO_RETRY_ENABLED=true

# Enable intent detection to prevent "I'll try X" without actually doing X
# When enabled, system detects when agent states intent but doesn't execute
# Default: true
LOGAI_INTENT_DETECTION_ENABLED=true

# === Logging Configuration ===
# Log level: DEBUG, INFO, WARNING, ERROR (default: INFO)
LOGAI_LOG_LEVEL=INFO

# Log file location (optional, defaults to stderr only)
# LOGAI_LOG_FILE=~/.logai/logai.log
