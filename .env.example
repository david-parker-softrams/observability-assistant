# LogAI Configuration
# Copy this file to .env and fill in your values

# === LLM Provider Configuration ===
# Choose your LLM provider: anthropic (default), openai, or ollama
LOGAI_LLM_PROVIDER=anthropic

# Anthropic API Key (required if using Anthropic)
# Get your key from: https://console.anthropic.com/
LOGAI_ANTHROPIC_API_KEY=your-anthropic-api-key-here

# OpenAI API Key (required if using OpenAI)
# Get your key from: https://platform.openai.com/api-keys
LOGAI_OPENAI_API_KEY=your-openai-api-key-here

# LLM Model Selection (optional, defaults to provider's recommended model)
LOGAI_ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
LOGAI_OPENAI_MODEL=gpt-4-turbo-preview

# === Ollama Configuration (Local Models) ===
# Base URL for Ollama API (default: http://localhost:11434)
LOGAI_OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use (must support function calling)
# Recommended: llama3.1:8b, llama3.1:70b, mistral
LOGAI_OLLAMA_MODEL=llama3.1:8b

# === AWS Configuration ===
# AWS Region for CloudWatch Logs
AWS_DEFAULT_REGION=us-east-1

# AWS Credentials (use one of the following methods)
# Method 1: Direct credentials
AWS_ACCESS_KEY_ID=your-aws-access-key-id
AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key

# Method 2: Use AWS profile (comment out access key/secret above)
# AWS_PROFILE=your-aws-profile-name

# === LogAI Application Settings ===
# Enable PII sanitization (default: true)
# When enabled, sensitive data like emails, IPs, API keys are redacted before sending to LLM
LOGAI_PII_SANITIZATION_ENABLED=true

# Cache directory for storing fetched logs (default: ~/.logai/cache)
LOGAI_CACHE_DIR=~/.logai/cache

# Maximum cache size in MB (default: 500)
LOGAI_CACHE_MAX_SIZE_MB=500

# Cache TTL in seconds for historical logs (default: 86400 = 24 hours)
LOGAI_CACHE_TTL_SECONDS=86400

# === Logging Configuration ===
# Log level: DEBUG, INFO, WARNING, ERROR (default: INFO)
LOGAI_LOG_LEVEL=INFO

# Log file location (optional, defaults to stderr only)
# LOGAI_LOG_FILE=~/.logai/logai.log
