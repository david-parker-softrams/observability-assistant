# Session Notes: Ollama Tool Calling & AWS Credentials Fix
**Date:** February 10, 2026  
**TPM:** George  
**Status:** ‚úÖ Successfully Completed

---

## Session Objective
Fix Ollama tool calling to enable local LLM access to CloudWatch tools, resolving the "Maximum tool iterations exceeded" error.

---

## Problem Statement

### Initial Issue
User reported that when running `logai`, the TUI would load but when trying any query, the system would hang with "Maximum tool iterations (10) exceeded" error.

### Root Causes Identified

#### 1. Ollama Tool Calling Disabled
**Problem:** Code at lines 139-141 and 226-228 in `litellm_provider.py` was filtering out tools for Ollama:
```python
# Only send tools to providers that support them (not Ollama)
if tools and self.provider in ["anthropic", "openai"]:
    params["tools"] = tools
```

**Why it existed:** Original assumption that Ollama didn't support tool calling.

**Reality:** Ollama has supported tool calling since July 2024!

#### 2. AWS Credentials Priority Issue
**Problem:** User had expired AWS temporary credentials in environment variables:
- `AWS_ACCESS_KEY_ID=ASIAU5LH5YCQNO7WAQQ6`
- `AWS_SECRET_ACCESS_KEY=...` (expired)
- `AWS_SESSION_TOKEN=...` (expired)

Even with `AWS_PROFILE=bosc-dev` set in `.env`, boto3 was using the expired environment credentials, causing:
> "UnrecognizedClientException: The security token included in the request is invalid"

---

## Research & Discovery

### Web Research on Ollama Tool Calling

**Key Findings:**
1. **Ollama Blog** (July 2024): Announced native tool calling support
2. **Supported Models:**
   - Llama 3.1+
   - Qwen 2.5/3 series
   - Mistral Nemo
   - Firefunction v2
   - Command-R+

3. **LiteLLM Integration:**
   - Use `ollama_chat/` prefix (not `ollama/`)
   - Sends requests to `/api/chat` endpoint (supports tools)
   - Can register models with `supports_function_calling: True`

**Documentation Sources:**
- https://ollama.com/blog/tool-support
- https://docs.litellm.ai/docs/providers/ollama
- https://github.com/ollama/ollama/blob/main/docs/api.md

---

## Solutions Implemented

### Fix 1: Enable Ollama Tool Calling (Commit 4112528)

**Files Modified:**
- `src/logai/providers/llm/litellm_provider.py`
- `tests/unit/test_llm_provider.py`

**Changes Made:**

1. **Registered Models with Function Calling** (lines 19-35)
```python
# Register Ollama models that support function calling
litellm.register_model(
    model_cost={
        "ollama_chat/qwen2.5": {"supports_function_calling": True},
        "ollama_chat/qwen3": {"supports_function_calling": True},
        "ollama_chat/llama3.1": {"supports_function_calling": True},
        "ollama_chat/llama3.2": {"supports_function_calling": True},
    }
)
```

2. **Changed Model Prefix** (line 292)
```python
# Before:
return f"ollama/{self.model}"

# After:
return f"ollama_chat/{self.model}"
```

3. **Added Tool Support Validation** (lines 113-129)
```python
def _supports_tools(self) -> bool:
    """Check if the current model supports tool calling."""
    if self.provider in ["anthropic", "openai"]:
        return True
    if self.provider == "ollama":
        model_name = self._get_model_name()
        supported_families = [
            "qwen2.5", "qwen3", "llama3.1", "llama3.2",
            "mistral-nemo", "firefunction"
        ]
        return any(f"ollama_chat/{family}" in model_name 
                   for family in supported_families)
    return False
```

4. **Updated Tool Sending Logic** (lines 175-177, 262-264)
```python
# Before:
if tools and self.provider in ["anthropic", "openai"]:
    params["tools"] = tools

# After:
if tools and self._supports_tools():
    params["tools"] = tools
```

5. **Updated Test** (line 291 in test file)
```python
# Before:
assert provider._get_model_name() == "ollama/llama3.1:8b"

# After:
assert provider._get_model_name() == "ollama_chat/llama3.1:8b"
```

**Code Review:** Billy gave 7/10 initially, improved to production-ready after addressing:
- Model registration being less version-specific
- Test expectations updated
- Validation added for tool support

---

### Fix 2: AWS Credentials Priority (Commit 7224b63)

**Files Modified:**
- `src/logai/providers/datasources/cloudwatch.py`

**Changes Made:**

**Before** (lines 64-73):
```python
if settings.aws_access_key_id and settings.aws_secret_access_key:
    client_kwargs["aws_access_key_id"] = settings.aws_access_key_id
    client_kwargs["aws_secret_access_key"] = settings.aws_secret_access_key

if settings.aws_profile:
    session = boto3.Session(profile_name=settings.aws_profile)
    self.client = session.client(**client_kwargs)  # BUG: Still uses explicit keys!
else:
    self.client = boto3.client(**client_kwargs)
```

**Issue:** Explicit credentials were added to `client_kwargs` first, then passed to session. Boto3 prioritizes explicit credentials over profile.

**After** (lines 51-79):
```python
# Credential priority: Profile > Explicit Keys > Default Chain
if settings.aws_profile:
    # Use a session with profile - ignores environment AWS_* variables
    session = boto3.Session(
        profile_name=settings.aws_profile,
        region_name=settings.aws_region
    )
    self.client = session.client("logs", config=self.config)
elif settings.aws_access_key_id and settings.aws_secret_access_key:
    # Explicit credentials provided
    client_kwargs["aws_access_key_id"] = settings.aws_access_key_id
    client_kwargs["aws_secret_access_key"] = settings.aws_secret_access_key
    self.client = boto3.client(**client_kwargs)
else:
    # Use default credential chain
    self.client = boto3.client(**client_kwargs)
```

**Result:** When `AWS_PROFILE` is set, boto3 now uses ONLY the profile credentials and ignores environment variables.

---

## Configuration Updates

### .env File Changes
```bash
# Updated model (user changed from qwen2.5:32b-instruct to qwen3:32b)
LOGAI_OLLAMA_MODEL=qwen3:32b

# AWS Profile (unchanged)
AWS_PROFILE=bosc-dev
AWS_DEFAULT_REGION=us-east-1
```

---

## Testing & Verification

### Test 1: Ollama Model Configuration
```bash
$ python -c "from logai.config import get_settings; ..."
Provider: ollama
Model: qwen3:32b
Model name: ollama_chat/qwen3:32b
Supports tools: True  ‚úÖ
```

### Test 2: AWS CloudWatch Access
Created diagnostic script `test_aws_access.py` (later removed) that verified:

**Before Fix:**
```
‚úó Failed to list log groups: UnrecognizedClientException - 
  The security token included in the request is invalid.
```

**After Fix:**
```
‚úì Successfully listed log groups
- Found 3 log groups:
  ‚Ä¢ /aws/apigateway/welcome
  ‚Ä¢ /aws/batch/bosc-aeptrackdatarefresh
  ‚Ä¢ /aws/batch/job
```

### Test 3: End-to-End TUI Test
User ran `logai` and confirmed:
> "It appears to be working now" ‚úÖ

---

## Technical Details

### Ollama Models Available on User's System
```
qwen3:32b            - 20 GB (SELECTED - has function calling) ‚≠ê
qwen2.5:32b-instruct - 19 GB (has function calling)
qwen2.5:7b           - 4.7 GB (backup option)
llama3.1:8b          - 4.9 GB (has function calling but smaller)
deepseek-r1:70b      - 42 GB (too large, reasoning model)
deepseek-r1:32b      - 19 GB (reasoning model)
llama3.2:latest      - 2.0 GB (too small)
```

### AWS Profile Configuration
```bash
# ~/.aws/credentials
[bosc-dev]
credential_process = /Users/David.Parker/go/bin/kion credential-process \
  --account-id 337909760160 \
  --cloud-access-role bosc-application-admin
```

**Role:** `ct-ado-bosc-application-admin`  
**Account:** 337909760160  
**Region:** us-east-1

---

## Key Learnings

### 1. Ollama Tool Calling
- **Not all Ollama models support tools** - must check compatibility
- **Use `ollama_chat/` prefix** for LiteLLM tool calling support
- **Model registration helps** but isn't strictly required (LiteLLM falls back to JSON mode)
- **Qwen 2.5/3 families** have excellent tool calling support

### 2. Boto3 Credentials Priority
- **Explicit credentials override profiles** even when profile is specified
- **Environment variables are implicit explicit credentials** via settings
- **Session with profile must be clean** - don't pass explicit credentials
- **Credential priority should be documented** and enforced in code

### 3. Debugging Process
- **Logs are essential** - checked textual.log, logai_startup.log
- **Environment matters** - expired AWS credentials were the hidden culprit
- **Test in isolation** - diagnostic scripts help identify exact issues
- **Web research first** - saved time by finding official documentation

---

## Files Changed Summary

### Commit 4112528: feat(llm): enable Ollama tool calling support with validation
```
src/logai/providers/llm/litellm_provider.py  | +42 -6
tests/unit/test_llm_provider.py              | +1 -1
```

### Commit 7224b63: fix(aws): prioritize AWS profile over environment credentials
```
src/logai/providers/datasources/cloudwatch.py | +18 -12
```

### Configuration Changes
```
.env | LOGAI_OLLAMA_MODEL changed from llama3.1:8b to qwen3:32b
```

---

## Test Results

### Unit Tests
```bash
$ pytest tests/unit/test_llm_provider.py
16/16 tests passed ‚úÖ
Coverage: 84% on litellm_provider.py
```

### Integration Tests
```bash
$ python test_aws_access.py
‚úì Settings loaded
‚úì Data source created
‚úì Successfully listed log groups
```

### End-to-End Test
```bash
$ logai
# User tested queries:
# - "List all my log groups" ‚Üí Worked! ‚úÖ
# - CloudWatch tool calling ‚Üí Successful! ‚úÖ
```

---

## Current System Status

### ‚úÖ Fully Functional Components
- TUI (Textual-based) - renders correctly
- Local LLM (Ollama + Qwen3:32b) - with tool calling
- CloudWatch integration - fetching logs
- AWS authentication - via profile
- PII sanitization - enabled
- Caching system - configured
- Tool calling - working end-to-end

### üìä Project Phase Status
- ‚úÖ Phase 1-3: Core infrastructure, PII sanitization
- ‚úÖ Phase 4: AWS CloudWatch Integration
- ‚úÖ Phase 5: LLM Integration with Tools
- ‚úÖ Phase 6: Caching System
- ‚úÖ Phase 7: TUI with Textual
- ‚úÖ **Phase 7 COMPLETE!** All features working end-to-end

### üéØ Next Steps (Optional)
- Phase 8: Integration testing & polish
- Phase 9: Advanced features (log insights, streaming, alerts)
- Phase 10: Documentation & distribution

---

## User Capabilities Now Available

The user can now perform:

1. **List log groups:**
   - "Show me all my log groups"
   - "List log groups starting with /aws/lambda"

2. **Fetch logs:**
   - "Get the latest logs from [log-group-name]"
   - "Show me logs from the last hour in [log-group]"

3. **Search and analyze:**
   - "Find errors in [log-group] from the last 24 hours"
   - "Analyze these logs and summarize the issues"

4. **Complex queries:**
   - "What's happening in my batch jobs?"
   - "Show me any security issues in my API Gateway logs"

---

## Recommended Documentation

For future users, consider creating `docs/local-setup.md`:

```markdown
# Running LogAI with Local Ollama

## Supported Models
- qwen2.5 (7b, 32b variants) - Best for tool calling
- qwen3 (all variants) - Excellent performance
- llama3.1, llama3.2 - Good tool support
- mistral-nemo, firefunction-v2 - Specialized

## Setup
1. Install Ollama: https://ollama.com
2. Pull a model: `ollama pull qwen3:32b`
3. Configure .env:
   ```
   LOGAI_LLM_PROVIDER=ollama
   LOGAI_OLLAMA_MODEL=qwen3:32b
   AWS_PROFILE=your-profile-name
   ```

## AWS Credentials
LogAI prioritizes credentials in this order:
1. AWS_PROFILE (highest priority)
2. Explicit AWS_ACCESS_KEY_ID/AWS_SECRET_ACCESS_KEY
3. Default boto3 credential chain

If you have expired credentials in your environment, AWS_PROFILE
will override them automatically.
```

---

## Session Metrics

**Duration:** ~2 hours  
**Commits:** 2 (both pushed to GitHub)  
**Tests:** All passing (16/16 unit tests)  
**Team Members:**
- George (TPM) - Coordination & research
- Jackie (Engineer) - Implementation
- Billy (Reviewer) - Code review

**Outcome:** ‚úÖ Complete Success - All features working

---

## Important Notes for Continuation

1. **Tool Calling Works!** Ollama with Qwen3:32b successfully calls CloudWatch tools
2. **AWS Profile Credentials** work correctly, even with expired environment variables
3. **Model Selection Matters** - Not all Ollama models support tool calling
4. **LiteLLM Prefix** - Must use `ollama_chat/` not `ollama/` for tool support
5. **Validation Added** - System checks if model supports tools before sending them

---

## References

**Ollama Documentation:**
- Blog: https://ollama.com/blog/tool-support
- Models: https://ollama.com/search?c=tools
- API Docs: https://github.com/ollama/ollama/blob/main/docs/api.md

**LiteLLM Documentation:**
- Ollama Provider: https://docs.litellm.ai/docs/providers/ollama
- Function Calling: https://docs.litellm.ai/docs/completion/function_call

**AWS Boto3:**
- Credentials: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html
- CloudWatch Logs: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/logs.html

---

**Session Completed Successfully!** üéâ
